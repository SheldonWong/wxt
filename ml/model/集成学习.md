
### GBDT实现参考
[gardient boosting from scratch](https://medium.com/mlreview/gradient-boosting-from-scratch-1e317ae4587d)
[GBDT](https://m.youtube.com/watch?v=sRktKszFmSk)
[kaggle master explains gradient boosting](http://blog.kaggle.com/2017/01/23/a-kaggle-master-explains-gradient-boosting/)
[C++实现GBDT](https://juejin.im/post/5b49505c6fb9a04fab44ff5b#heading-9)

[xgb探索，包括输出值是什么，叶子情况等](https://blog.csdn.net/kizgel/article/details/78261672)
[xgb输出](https://blog.csdn.net/CY_TEC/article/details/80209453)

###
[随机森林，GBDT，XGBoost的对比](https://blog.csdn.net/yingfengfeixiang/article/details/80210145)
[GBDT算法详解](https://www.zybuluo.com/Dounm/note/1031900#5-xgboost%E7%9A%84%E4%BC%98%E5%8C%96)
[Boosted Trees 介绍](http://xgboost.apachecn.org/cn/latest/model.html)

### GBDT的实现

### 训练过程
- 假设第一颗树是以ground truth为目标构建的；那么：
- 第二颗树是以（truth - 第一棵树的结果）为目标构建的；
- 第N颗树是以（truth - 前面n-1颗树之和）为目标构建的

### 基树
- 基树如何学习？

### 预测过程
输出是把每棵树的预测值加在一起，这也MART(muliple additive regression trees)得名的由来，预测过程树之间并没有依赖，不存在先算后算的问题，所以可以并行。

### 实现的时候，如何验证每一步的正确性（其他算法也一样）？

### xgboost输出学习的树

